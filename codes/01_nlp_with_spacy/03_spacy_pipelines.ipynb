{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247cddd9",
   "metadata": {},
   "source": [
    "## Spacy Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b1a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component     Assigns               Requires   Scores    Retokenizes\n",
      "-   -----------   -------------------   --------   -------   -----------\n",
      "0   sentencizer   token.is_sent_start              sents_f   False      \n",
      "                  doc.sents                        sents_p              \n",
      "                                                   sents_r              \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"This is sentence one. This is sentence two. This is sentence three. This continues for thousands of sentences.\"\n",
    "\n",
    "# Creating blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Adding component to blank model\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(text)\n",
    "sentences = [sent for sent in doc.sents]\n",
    "sentences\n",
    "    # [This is sentence one., This is sentence two., This is sentence three., This continues for thousands of sentences.]\n",
    "\n",
    "\n",
    "nlp.analyze_pipes(pretty=True)\n",
    "\n",
    "\"\"\"\n",
    "============================= Pipeline Overview =============================\n",
    "\n",
    "#   Component     Assigns               Requires   Scores    Retokenizes\n",
    "-   -----------   -------------------   --------   -------   -----------\n",
    "0   sentencizer   token.is_sent_start              sents_f   False      \n",
    "                  doc.sents                        sents_p              \n",
    "                                                   sents_r              \n",
    "\n",
    "✔ No problems found.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "'summary': {\n",
    "    'sentencizer': {\n",
    "        'assigns': ['token.is_sent_start', 'doc.sents'],\n",
    "        'requires': [],\n",
    "        'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
    "        'retokenizes': False\n",
    "        }\n",
    "    },\n",
    "    'problems': {\n",
    "        'sentencizer': []\n",
    "    },\n",
    "    'attrs': {\n",
    "        'token.is_sent_start': {\n",
    "            'assigns': ['sentencizer'], \n",
    "            'requires': []\n",
    "        },\n",
    "        'doc.sents': {\n",
    "            'assigns': ['sentencizer'], 'requires': []\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component     Assigns               Requires   Scores          Retokenizes\n",
      "-   -----------   -------------------   --------   -------------   -----------\n",
      "0   sentencizer   token.is_sent_start              sents_f         False      \n",
      "                  doc.sents                        sents_p                    \n",
      "                                                   sents_r                    \n",
      "                                                                              \n",
      "1   ner           doc.ents                         ents_f          False      \n",
      "                  token.ent_iob                    ents_p                     \n",
      "                  token.ent_type                   ents_r                     \n",
      "                                                   ents_per_type              \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': [], 'ner': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER focused pipeline example\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Analyzing pipeline components\n",
    "analysis = nlp.analyze_pipes(pretty = True)\n",
    "analysis\n",
    "\n",
    "\"\"\"\n",
    "============================= Pipeline Overview =============================\n",
    "\n",
    "#   Component     Assigns               Requires   Scores          Retokenizes\n",
    "-   -----------   -------------------   --------   -------------   -----------\n",
    "0   sentencizer   token.is_sent_start              sents_f         False      \n",
    "                  doc.sents                        sents_p                    \n",
    "                                                   sents_r                    \n",
    "                                                                              \n",
    "1   ner           doc.ents                         ents_f          False      \n",
    "                  token.ent_iob                    ents_p                     \n",
    "                  token.ent_type                   ents_r                     \n",
    "                                                   ents_per_type              \n",
    "\n",
    "✔ No problems found.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Example: Create NER-focused pipeline\n",
    "nlp_ner = spacy.blank(\"en\")\n",
    "nlp_ner.add_pipe(\"ner\")\n",
    "\n",
    "# Analyze pipeline components\n",
    "print(\"\\nPipeline Analysis:\")\n",
    "analysis = nlp_full.analyze_pipes(pretty=True)\n",
    "\n",
    "# Alternative: Analyze custom pipeline\n",
    "print(\"\\nCustom Pipeline Analysis:\")\n",
    "custom_analysis = nlp_blank.analyze_pipes(pretty=True)\n",
    "\n",
    "# Example: Adding multiple components\n",
    "nlp_custom = spacy.blank(\"en\")\n",
    "nlp_custom.add_pipe(\"sentencizer\")\n",
    "nlp_custom.add_pipe(\"ner\")\n",
    "\n",
    "print(\"Multi-component custom pipeline:\", nlp_custom.pipe_names)\n",
    "\n",
    "# Process text and access both sentences and entities (if trained)\n",
    "doc_multi = nlp_custom(text)\n",
    "print(\"Sentences:\", [sent.text.strip() for sent in doc_multi.sents])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
