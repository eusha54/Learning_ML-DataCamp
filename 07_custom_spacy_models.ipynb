{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbf2a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Oxford Street\n",
      "ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The car was navigating to the Oxford Street\")\n",
    "    # text containing locality entity \"Oxford Street\"\n",
    "\n",
    "# The issue with pretrained model, doesn't recognize Oxford Street as Location entity\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)\n",
    "    print(ent.label_)\n",
    "\"\"\"\n",
    "    the Oxford Street\n",
    "    ORG\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808b4a6",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc16137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Patient', 'was', 'prescribed', 'Aspirin'], 'SPACY': [True, True, True, False], 'TAG': ['', '', '', ''], 'LEMMA': ['', '', '', ''], 'POS': ['', '', '', ''], 'MORPH': ['', '', '', ''], 'HEAD': [0, 1, 2, 3], 'DEP': ['', '', '', ''], 'SENT_START': [1, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp.make_doc(\"Patient was prescribed Aspirin\")\n",
    "doc2 = nlp.make_doc(\"Bill Gates visited SFO Airport\")\n",
    "\n",
    "annotation1 = {\"entities\": [(24, 31, \"MEDICINE\")]}\n",
    "annotation2 = {\"entities\": [(0, 10, \"PERSON\"), (19, 29, \"LOC\")]}\n",
    "\n",
    "example1 = Example.from_dict(doc1, annotation1)\n",
    "example2 = Example.from_dict(doc2, annotation2)\n",
    "\n",
    "print(example1.to_dict())\n",
    "\"\"\"\n",
    "{\n",
    "'doc_annotation': {\n",
    "    'cats': {}, \n",
    "    'entities': ['O', 'O', 'O', 'O'], \n",
    "    'spans': {}, \n",
    "    'links': {}\n",
    "    },\n",
    "'token_annotation': {\n",
    "    'ORTH': ['Patient', 'was', 'prescribed', 'Aspirin'], \n",
    "    'SPACY': [True, True, True, False], \n",
    "    'TAG': ['', '', '', ''], \n",
    "    'LEMMA': ['', '', '', ''], \n",
    "    'POS': ['', '', '', ''], \n",
    "    'MORPH': ['', '', '', ''], \n",
    "    'HEAD': [0, 1, 2, 3], \n",
    "    'DEP': ['', '', '', ''], \n",
    "    'SENT_START': [1, 0, 0, 0]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773db133",
   "metadata": {},
   "source": [
    "## Training with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the NER pipeline\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Example training data (you can add more sentences here)\n",
    "training_data = [\n",
    "    (\"Barack Obama visited Microsoft headquarters in Seattle on Monday.\", {\n",
    "        \"entities\": [\n",
    "            (0, 12, \"PERSON\"),      # Barack Obama\n",
    "            (21, 30, \"ORG\"),        # Microsoft\n",
    "            (47, 54, \"LOC\"),        # Seattle\n",
    "            (58, 64, \"DATE\")        # Monday\n",
    "        ]\n",
    "    }),\n",
    "    (\"Bill Gates founded Microsoft.\", {\n",
    "        \"entities\": [\n",
    "            (0, 10, \"PERSON\"),\n",
    "            (19, 28, \"ORG\")\n",
    "        ]\n",
    "    }),\n",
    "    (\"Elon Musk is the CEO of SpaceX.\", {\n",
    "        \"entities\": [\n",
    "            (0, 9, \"PERSON\"),\n",
    "            (27, 33, \"ORG\")\n",
    "        ]\n",
    "    })\n",
    "]\n",
    "\n",
    "# Add new labels to the NER\n",
    "for _, annotations in training_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipes except NER\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    # Create optimizer\n",
    "    optimizer = nlp.resume_training()\n",
    "\n",
    "    # Number of iterations\n",
    "    epochs = 20\n",
    "\n",
    "    for i in range(epochs):\n",
    "        losses = {}\n",
    "        random.shuffle(training_data)\n",
    "        for text, annotation in training_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotation)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "        print(f\"Epoch {i+1}/{epochs} - Losses: {losses}\")\n",
    "\n",
    "# Save model to disk\n",
    "nlp.to_disk(\"custom_ner\")\n",
    "\n",
    "print(\"Training complete! Model saved as 'custom_ner'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd2b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd11999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yafas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Elon Musk is the CEO of SpaceX.\" with entities \"[(0, 9, 'PERSON'), (27, 33, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example training data\n",
    "#   list of tuples of the form\n",
    "#   2 values in tuples - \n",
    "#      sentence string\n",
    "#      entities dictionary\n",
    "\"\"\"\n",
    "    (\"Sentence\", {\n",
    "        \"entities\": [(start, end, \"label), ...]\n",
    "    })\n",
    "\n",
    "\"\"\"\n",
    "training_data = [\n",
    "    (\"Barack Obama visited Microsoft headquarters in Seattle on Monday.\", {\n",
    "        \"entities\": [\n",
    "            (0, 12, \"PERSON\"),      # Barack Obama\n",
    "            (21, 30, \"ORG\"),        # Microsoft\n",
    "            (47, 54, \"LOC\"),        # Seattle\n",
    "            (58, 64, \"DATE\")        # Monday\n",
    "        ]\n",
    "    }),\n",
    "    (\"Bill Gates founded Microsoft.\", {\n",
    "        \"entities\": [\n",
    "            (0, 10, \"PERSON\"),\n",
    "            (19, 28, \"ORG\")\n",
    "        ]\n",
    "    }),\n",
    "    (\"Elon Musk is the CEO of SpaceX.\", {\n",
    "        \"entities\": [\n",
    "            (0, 9, \"PERSON\"),\n",
    "            (27, 33, \"ORG\")\n",
    "        ]\n",
    "    })\n",
    "]\n",
    "\n",
    "# Getting the ner pipe\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Disabling Other Pipes\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# Creating Optimizer\n",
    "optmizer = nlp.resume_training()\n",
    "\n",
    "# Updating model weights\n",
    "losses = {}\n",
    "epochs = 20\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(training_data)\n",
    "    for text, annotation in training_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotation)\n",
    "        nlp.update([example], sgd = optmizer, losses = losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db706365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# Load pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable other components except NER\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    # Sample training data\n",
    "    TRAIN_DATA = [\n",
    "        {\n",
    "            \"sentence\": \"Patient was prescribed Aspirin.\",\n",
    "            \"entities\": [(24, 31, \"MEDICINE\")]\n",
    "        }\n",
    "    ]\n",
    "    examples = []\n",
    "    for record in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(record[\"sentence\"])\n",
    "        annotations = {\"entities\": record[\"entities\"]}\n",
    "        examples.append(Example.from_dict(doc, annotations))\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = nlp.create_optimizer()\n",
    "    losses = {}\n",
    "\n",
    "    # Train for 5 epochs\n",
    "    for epoch in range(5):\n",
    "        random.shuffle(examples)\n",
    "        nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "        print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
    "# Output (example):\n",
    "# Epoch 1, Losses: {'ner': 0.8}\n",
    "# Epoch 2, Losses: {'ner': 0.5}\n",
    "# Epoch 3, Losses: {'ner': 0.3}\n",
    "# Epoch 4, Losses: {'ner': 0.2}\n",
    "# Epoch 5, Losses: {'ner': 0.1}\n",
    "\n",
    "# Save NER component\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.to_disk(\"ner_model\")\n",
    "\n",
    "# Load trained NER component\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "ner2 = nlp2.create_pipe(\"ner\")\n",
    "ner2.from_disk(\"ner_model\")\n",
    "nlp2.add_pipe(ner2, name=\"ner\")\n",
    "\n",
    "# Inference\n",
    "doc = nlp2(\"Aspirin was given to the patient.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# Output:\n",
    "# Aspirin MEDICINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18a450",
   "metadata": {},
   "source": [
    "## Customizing Spacy Model DataCamp example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a tuple of (entities text, entities label) if Jumbo is in the entity\n",
    "target_entities = []\n",
    "for doc in documents:\n",
    "  target_entities.extend([(ent.text, ent.label_) for ent in doc.ents if \"Jumbo\" in ent.text])\n",
    "print(target_entities)\n",
    "\n",
    "# Append True to the correct_labels list if the entity label is `PRODUCT`\n",
    "correct_labels = []\n",
    "for ent in target_entities:\n",
    "  if target_entities[1] == \"PRODUCT\":\n",
    "    correct_labels.append(True)\n",
    "  else:\n",
    "    correct_labels.append(False)\n",
    "print(correct_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"A patient with chest pain had hyperthyroidism.\"\n",
    "entity_1 = \"chest pain\"\n",
    "entity_2 = \"hyperthyroidism\"\n",
    "\n",
    "# Store annotated data information in the correct format\n",
    "annotated_data = {\"sentence\": text, \"entities\": [{\"label\": \"SYMPTOM\", \"value\": entity_1}, {\"label\": \"DISEASE\", \"value\": entity_2}]}\n",
    "\n",
    "# Extract start and end characters of each entity\n",
    "entity_1_start_char = text.find(entity_1)\n",
    "entity_1_end_char = entity_1_start_char + len(entity_1)\n",
    "entity_2_start_char = text.find(entity_2)\n",
    "entity_2_end_char = entity_2_start_char + len(entity_2)\n",
    "\n",
    "# Store the same input information in the proper format for training\n",
    "training_data = [(text, {\"entities\": [(entity_1_start_char,entity_1_end_char,\"SYMPTOM\"), \n",
    "                                      (entity_2_start_char,entity_2_end_char,\"DISEASE\")]})]\n",
    "print(training_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example_text = 'A patient with chest pain had hyperthyroidism.'\n",
    "training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
    "\n",
    "all_examples = []\n",
    "# Iterate through text and annotations and convert text to a Doc container\n",
    "for text, annotations in training_data:\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Create an Example object from the doc contianer and annotations\n",
    "  example_sentence = Example.from_dict(doc, annotations)\n",
    "  print(example_sentence.to_dict(), \"\\n\")\n",
    "  \n",
    "  # Append the Example object to the list of all examples\n",
    "  all_examples.append(example_sentence)\n",
    "  \n",
    "print(\"Number of formatted training data: \", len(all_examples))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable all pipeline components of  except `ner`\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# Convert a text and its annotations to the correct format usable for training\n",
    "doc = nlp.make_doc(text)\n",
    "example = Example.from_dict(doc, annotations)\n",
    "print(\"Example object for training: \\n\", example.to_dict())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Before training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "# Shuffle training data and the dataset using random package per epoch\n",
    "for i in range(epochs):\n",
    "  random.shuffle(training_data)\n",
    "  for text, annotations in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    # Update nlp model after setting sgd argument to optimizer\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], sgd = optimizer)\n",
    "print(\"After training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a blank English model, add NER component, add given labels to the ner pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for ent in labels:\n",
    "    ner.add_label(ent)\n",
    "\n",
    "# Disable other pipeline components, complete training loop and run training loop\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "for text, annotation in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotation)\n",
    "    nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
